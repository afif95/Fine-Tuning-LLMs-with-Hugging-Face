# -*- coding: utf-8 -*-
"""ABK - Fine-Tuning LLMs with Hugging Face

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PVOEumWOlHtwrKcJXkpA2jhedk9H4TYW

# Fine-Tuning LLMs with Hugging Face

## Step 1: Installing and importing the libraries
"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

!pip install huggingface_hub

import torch
from trl import SFTTrainer
from peft import LoraConfig
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)

"""## Step 2: Loading the model"""

# using 4 bit precision to reduce the model size and speed up the inference
# getattr is used to fetch the float16 data type from the torch module
# 2 pytorch models will be loaded by loading the fine tuned llama2 model
# nf4 quantization data types in the weights of the linear layers of the llm
llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2",
                                                   quantization_config = BitsAndBytesConfig(load_in_4bit = True,
                                                                                            bnb_4bit_compute_dtype = getattr(torch, "float16"),
                                                                                            bnb_4bit_quant_type = "nf4"))
# not storing the output of the previously computed layers in the cache memory to reduce memory usage and speed up training computation
llama_model.config.use_cache = False
# deactivate more accurate computation of the linear layers to avoid slowing down the linear layers' computation
llama_model.config.pretraining_tp = 1

"""## Step 3: Loading the tokenizer"""

# trust the model
llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2", trust_remote_code = True)
# all the sequences will be of the same length each one being filled up by the padding token here the end of string token
llama_tokenizer.pad_token = llama_tokenizer.eos_token
# right padding
llama_tokenizer.padding_side = "right"

"""## Step 4: Setting the training arguments"""

training_arguments = TrainingArguments(output_dir = "./results", per_device_train_batch_size = 4, max_steps = 100)

"""## Step 5: Creating the Supervised Fine-Tuning trainer"""

# main objective: augmenting the knowledge of LLM
# 2 techniques:
# 1. Supervised Fine-Tuning: transfer learning technique in which the weights of a pretrained model are trained on new data
# 2. RLHF(Reinforcement Learning from Human Feedback): requires human feedback through reinforcement learning (more complex)
# peft(parameter efficient fine tuning) --> minimal fine tuning of the parameters to avoid training all the parameters as the model is huge
# r --> low rank matrices of minimal parameters (trainable efficient parameters)
# CAUSAL_LM --> CAUSAL Language Model
# lora_dropout --> deactivate some of the parameters/weights during the training so they are not updated all the time; regularization technique to prevent overfitting
llama_sft_trainer = SFTTrainer(model = llama_model,
                               args = training_arguments,
                               train_dataset = load_dataset(path = "aboonaji/wiki_medical_terms_llam2_format", split = "train"),
                               tokenizer = llama_tokenizer,
                               peft_config = LoraConfig(task_type = "CAUSAL_LM", r = 64, lora_alpha = 16, lora_dropout = 0.1),
                               dataset_text_field = "text")

"""## Step 6: Training the model"""

llama_sft_trainer.train()

"""## Step 7: Chatting with the model"""

user_prompt = "Please tell me about Choriocarcinoma"
text_generation_pipeline = pipeline(task = "text-generation", model = llama_model, tokenizer = llama_tokenizer, max_length = 300)
model_answer = text_generation_pipeline(f"<s>[INST] {user_prompt} [/INST]")
print(model_answer[0]['generated_text'])